---
layout: default
title: home
permalink: /
---
<h1>Machine Learning Research Unit <!--of {{site.data.links.TUWienInformatics}}--></h1>

Welcome to the web-home of the new Machine Learning research unit of {{site.data.links.TUWienInformatics}}. 

<br><br>
	<table style="border-collapse:collapse;border-color:rgb(136,136,136);border-width:0px" cellspacing="0" bordercolor="#888" border="0">
<tbody>
<tr>
<td>
<div style="display:block;text-align:left"><img src="files/tuwien.svg" border="0" width="200" height="199"></div>
</td>
<td style="width:60px;height:255px">&nbsp;</td>
<td style="width:200px;height:255px">
<h3>
<div style="display:block;text-align:left"><img src="files/colour_1_ml.svg" border="0" width="200" height="199"></div>
</h3>
</td>
<td style="width:60px;height:255px">&nbsp;</td>
<td style="width:500px;height:255px">
Machine Learning Research Unit (E 194-06)<br><br><br>
Information Systems Engineering Institute<br>
Faculty of Informatics<br>
TU Wien<br><br>
<br>
<!--E-Mail: mail&nbsp; ... &nbsp; thomasgaertner.org<br>-->
<br>
Web:<a href="https://ml-tuw.github.io/"> https://ml-tuw.github.io/</a> <br>
Physical: Erzherzog-Johann-Platz 1 (FB02), 1040 Vienna, Austria
</td>
</tr>
</tbody>
</table>

<h3> News </h3>
<ol>
<li><a href="https://www.kth.se/profile/neum">Stefan Neumann</a> received funding for a <i>Vienna Research Group</i> and will join our research unit in 2024. (October'23)</li>
<li>Our paper <i>Expressivity-Preserving GNN Simulation</i> was accepted at <b>NeurIPS'23.</b> (October'23)</li>
<li>We won the <a href="publications/expectation_complete_poster_ICML.pdf">best poster</a> award at G-Research's ICML poster party in London where our very own Max Thiessen presented <i>Expectation-Complete Graph Representations with Homomorphisms</i>.</li>
<li>We hired two amazing new PostDocs! Dr. {% include listppl.html ppls="PascalWelke" -%} on our <a href="projects/">StruDL</a> project and Dr. {% include listppl.html ppls="SagarMalhotra" -%}. Welcome to the team! (May '23)</li>
<li>Our paper <i><a href="https://arxiv.org/abs/2208.11561">Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions</a></i> has been accepted at <b>IJCAI'23</b>. It has also been accepted for spotlight presentation at the <b>NeSy'23</b> workshop. (May '23)	
<li>Our paper <i><a href="https://arxiv.org/abs/2306.15786">An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning</a></i> got accepted at <b>ECMLPKDD'23</b>. (May '23)</li>
<li>Two accepted papers at <b>ACL'23</b>! <i><a href="https://arxiv.org/abs/2207.03777">Hidden Schema Networks</a></i> and <i><a href="https://arxiv.org/abs/2209.01106">A New Aligned Simple German Corpus</a></i>. (May '23)</li>
<li>Our paper on <i>Expectation-Complete Graph Representations with Homomorphisms</i> has been accepted at <b>ICML'23</b>! (April '23)</li>
<li>Our paper <i>Can stochastic weight averaging improve generalization in private learning?</i> has been accepted at the RTML workshop of <b>ICLR'23</b>! (April '23)</li>
<li>We are organising the <a href="https://mlg-europe.github.io" target=_blank>Mining and Learning with Graphs (MLG) workshop</a> this year at <a href="https://2023.ecmlpkdd.org/">ECMLPKDD2023</a>. (April '23)</li>
<li>We organised the <i>1st community event for students of learning algorithms in Vienna</i> (<a href="https://cestlawien.github.io">C'est La Wien</a>, Feb '23)</li>
<li><a href="https://sailab.diism.unisi.it/people/caterina-graziani/">Caterina Graziani (Universit√† degli Studi di Siena)</a> is visiting us for four months! 
(Feb '23)</li>
<li>Our paper on <i><a href=https://pubs.rsc.org/en/content/articlelanding/2023/DD/D3DD00004D>Krein support vector machine classification of antimicrobial peptides</a></i> has been accepted for publication in the <i>Digital Discovery</i> journal! Joint work of  Joseph Redshaw, Darren S. J. Ting, Alex Brown, Jonathan D. Hirst, and {%include linked_name.html id="ThomasGaertner"%}. (Feb '23)
</li> 
<li>Two of our papers have been accepted at <b>NeurIPS'22</b>! (Oct '22)</li>
<li>Our very own Fabian Jogl won the (community voted) Best Poster Award at the MLG@ECMLPKDD Workshop. Congratulations! (Sep '22) </li>
<li>We organised machine learning courses for children at the <a href="https://www.kinderuni-anmeldung.at/index.php?field_id=&amp;ts=&amp;token=fce978a8f469217c8ea2e5c182f2cdfc&amp;event_age_type_id=&amp;event_type=&amp;event_sub_type=38&amp;keywords=&amp;page=" target="_blank" rel="noopener">KinderUni Wien</a>. (Jul '22)</li>
<li>Together with <a href="https://mlai.cs.uni-bonn.de/people/pascal-welke" target=_blank>Pascal Welke</a> from the University of Bonn, we are organising the <a href="https://mlg-europe.github.io/2022/" target=_black>Mining and Learning with Graphs (MLG) workshop</a> this year at <a href="https://2022.ecmlpkdd.org/" target=_blank>ECMLPKDD'22</a>. <b><a href="https://openreview.net/group?id=ecmlpkdd.org/ECMLPKDD/2022/Workshop/MLG" target=_blank>Submit your work until June 20th!</a></b></li>
<li>Our very own {% include linked_name.html id="TamaraDrucks" %} won the <a href="https://informatics.tuwien.ac.at/epilog/best-poster-award/">Best Poster Award</a> of TU Wien Informatics. Congratulations! (Jan '22)<br></li>
<li>Our paper on <i><a href=https://pubs.acs.org/doi/10.1021/acs.jcim.1c00699>Kernel Methods for Predicting Yields of Chemical Reactions</a></i> has been accepted for publication in the <i>Journal of Chemical Information and Modeling</i>. Joint work of Alexe Haywood, Joseph Redshaw, Magnus Hanson-Heine, Adam Taylor, Alex  Brown, Andrew Mason, {% include linked_name.html id="ThomasGaertner" %}, and Jonathan Hirst. (Oct '21)
</li>
<li>
Our paper on <i>Conditional Network Data Balancing With GANs</i> has been accepted at the <i>NeurIPS Deep Generative Models and Downstream Applications Workshop</i>. Joint work ok Fares Meghdouri, Thomas Schmied, Tanja Zseby, and {% include linked_name.html id="ThomasGaertner" %}. (Oct '21)
</li>
<li>Our paper on <i><a href=https://drive.google.com/file/d/1PZtXR8-o2qzuU2CnpW319LDI1mBIbR4e/view?usp=sharing>Active Learning Convex Halfspaces on Graphs</a></i> (<a href=https://drive.google.com/file/d/1ih__zFB1HYqBStNyizzN_raRN9042kzI/view?usp=sharing>supplementary material</a>) has been accepted at <b>NeurIPS'21</b>. Congratulations to  {% include linked_name.html id="MaxThiessen" %} for his first NeurIPS paper! (Sept '21)<br> </li>
<li>Our team won the prize for the <i>Best Practical Result</i> in the <a href="https://ecosystem.siemens.com/topic/detail/default/33/overview">Siemens AI-Dependability Assessment Challenge</a>. Congratulations to {% include linked_name.html id="JosephRedshaw" %} (University of Nottingham), {% include linked_name.html id="MaxThiessen" %} (TU Wien), and {% include linked_name.html id="DavidPenz" %} (TU Wien)! (July '21)<br></li>
<li>Our very own {% include linked_name.html id="DavidPenz" %} won the <a href="https://informatics.tuwien.ac.at/epilog/distinguished-young-alumn/">Distinguished Young Alumn award</a> of TU Wien Informatics. Congratulations! (June '21)<br></li>
<li>FFG has aggreed to fund our proposal on <i>Artificial Intelligence for Advanced SAR Processing</i>! (May '21)</li>
<li>TU Wien has agreed to fund <a href="https://secint.visp.wien/people/">our</a> doctoral college on <i><a href="https://secint.visp.wien/">Secure and Intelligent Human-Centric
Digital Technologies </a></i>! (July '20)</li>
<li>Founding of the Machine Learning research unit. (Feb '20)</li>
<li>BBSRC has aggreed to fund our proposal on <i>Bacteriophage to control Salmonella in pigs</i>! (Oct '19)</li>
</ol>

<h3>Topics</h3>
	Our research aims to narrow the gap between theoretically well-understood and practically relevant machine learning. Research questions concern for instance: 
	<ul>
	<li>learning with non-conventional data, i.e., data that has no inherent representation in a table or Euclidean space</li>
	<li>incorporation of invariances as well as expert domain knowledge in learning algorithms</li>
	<li>computational, sample, query, and communication complexity of learning algorithms</li>
	<li>constructive machine learning scenarios such as structured output prediction</li>
	<li>learning with small labelled data sets and large unlabelled data sets</li>
	<li>adversarial learning with mistake and/or regret bounds</li>
	<li>parallelisation/distribution of learning algorithms</li>
    <li>approximation of learning algorithms</li>
	<li>scalability of learning algorithms</li>
	<li>reliability of learning algorithms</li>
	<li>extreme learning</li>
	<li>...</li>
	</ul>
	To demonstrate the practical effectiveness of novel learning algorithms, we apply them in Chemistry, Material Science, Electrical Engineering, Computer Games, Humanities, etc.
<br><br><br><br>
