---
layout: entitled
title: Generalisation
---

**Motivation**: The ability of a model to adapt and perform well on new data is crucial. A model which generalises not only performs well on the training set, but on unseen data as well. Understanding and characterising why and how deep learning can generalise well is still an open question.

**Overview**:

- notes on generalisation (Prof. Roger Grosse) [(link)](https://www.cs.toronto.edu/~lczhang/321/notes/notes09.pdf)
- generalisation and overfitting [(youtube-link)](https://www.youtube.com/watch?v=pFWiauHOFpY)

**Papers and topics**:

- memorisation (Arpit, et al. "A closer look at memorization in deep networks." ICML 2017)
- double-descent (Belkin, et al. "Reconciling modern machine-learning practice and the classical biasâ€“variance trade-off." Proceedings of the National Academy of Sciences 2019)
- generalisation gap (Keskar, et al. "On large-batch training for deep learning: Generalization gap and sharp minima." ICLR 2017)
- loss landscape (Fort and Jastrzebski. "Large scale structure of neural network loss landscapes." NeurIPS 2019 **and** Li, et al. "Visualizing the loss landscape of neural nets." NeurIPS 2018)

