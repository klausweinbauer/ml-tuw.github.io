---
layout: entitled
title: Kernel Methods
---

**Motivation**: Kernels generalise linear classifiers to linear functions in a (potentially infinite dimensional) feature space. They are the foundation of various popular machine learning algorithms like the kernel SVM and kernel PCA.

**Overview**:

- chapters 1 and 2 of "Learning with kernels" by Bernhard Schölkopf and Alex Smola, 2002 [(pdf)](http://agbs.kyb.tuebingen.mpg.de/lwk/)
- introduction to kernels: Bernhard Schölkopf - MLSS 2013 [(youtube-link)](https://www.youtube.com/watch?v=uzWgB1VO9xQ)

**Papers and topics**:

- Nyström method (Drineas and Mahoney. "On the Nyström method for approximating a Gram matrix for improved kernel-based learning." Journal of machine learning research 2005 **and** Kumar, et al. "Sampling methods for the Nyström method." Journal of machine learning research 2012)
- Nyström method with kernel k-means++ samples as landmarks (Drineas and Mahoney. "On the Nyström method for approximating a Gram matrix for improved kernel-based learning." Journal of machine learning research 2005 **and** Oglic and Gärtner. "Nyström method with kernel k-means++ samples as landmarks." ICML 2017)
- random features (Rahimi and Recht. "Random features for large-scale kernel machines." NIPS 2007 **and** Le, et al. "Fastfood: approximate kernel expansions in loglinear time." ICML 2013)
- neural tangent kernel (Jacot, et al. "Neural tangent kernel: convergence and generalization in neural networks." NIPS 2018)
