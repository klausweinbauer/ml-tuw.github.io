---
layout: entitled
title: Optimisation (and Generalisation) in Neural Networks
---

**Overview**:

- A. Globerson: How SGD Can Succeed Despite Non-Convexity and Over-Parameterization [(slides)](https://simons.berkeley.edu/sites/default/files/docs/9983/simonsjune18.pdf)

**Papers and topics**:

- generalization bounds for deep neural networks (G.K. Dziugaite, D.M. Roy, "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data", 2017)
- why SGD avoids overfitting and finds global minima (A. Brutzkus et al: "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", 2017)
- connection between flatness of loss curve and generalisation (H. Petzka et al. "Relative Flatness and Generalization", 2021)
- mode connectivity (Garipov, et al. "Loss surfaces, mode connectivity, and fast ensembling of dnns." NeurIPS 2018).
- deep learning and generalisation (Zhang, et al. "Understanding deep learning (still) requires rethinking generalization." Communications of the ACM, 2021)
- connectivity of the optimisation landscape (A. Shevchenko, M. Mondelli. "Landscape Connectivity and Dropout Stability of SGD Solutions for Over-parameterized Neural Networks", 2020)
- SGD stability (Hardt, Moritz, Ben Recht, and Yoram Singer. "Train faster, generalize better: Stability of stochastic gradient descent." International conference on machine learning. PMLR, 2016)
- choose one or more papers listed on page 14 in the above mentioned [slides](https://simons.berkeley.edu/sites/default/files/docs/9983/simonsjune18.pdf) :)
