---
layout: entitled
title: Modern aspects of learning theory
---

**Motivation**: Learning theory studies computational and algorithmic aspects of machine learning algorithms to prove guarantees such as sample complexity bounds. This important to understand and devise novel learning algorithms. In recent years, many long-standing open questions in learning theory have been answered.

**Overview**:

- Olivier Bousquet Stéphane Boucheron, and Gábor Lugosi: "Introduction to Statistical Learning Theory" 2003.
- Chapters 1-6 of "Understanding machine learning"
- "Extending Generalization Theory Towards Addressing Modern Challenges in ML" by Shay Moran, talk at the HUJI ML Club, 2021 [(youtube-link)](https://www.youtube.com/watch?v=E6Umv6XBJck)
- (Basic material) Statistical Machine Learning by Ulrike von Luxburg (we recommend part 38-41) [(youtube playlist)](https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC)

**Papers and topics**:

- partial concept classes (Alon, et al., "A theory of PAC learnability of partial concept classes", unpublished arXiv:2107.08444)
- tight bounds (Bousquet, et al., "Proper learning, Helly number, and an optimal SVM bound" COLT 2020)
- universal learning (Bousquet, et al., "A theory of universal learning" STOC 2021)
- sample compression schemes (Moran, et al., "Sample compression schemes for VC classes" Journal of the ACM 2016)
- algorithmic stability (Bousquet, Olivier, and André Elisseeff. "Algorithmic stability and generalization performance." NeurIPS, 2000)
