---
layout: entitled
title: Calibration in Neurosymbolic Models

---

**Type**: Theory and experiments 

**Context**: 
Are probabilities from neuro-symbolic (NeSy) models **calibrated**?
Focus on the trade-off between **probability calibration** and **constraint satisfaction** in **Semantic Loss** and **BEARS**.
NeSy methods enforce rules or encourage knowledge-aware uncertainty. This often improves logical consistency or reduces overconfidence, but may skew predicted probabilities away from observed frequencies. The seminar clarifies **when** these methods help or hurt calibration and how to measure the trade-off.


**Suggested approach**: 
* [Theory] Analyze when calibration and constraint satisfaction are mutually compatible objectives.
* [Experiments] Check calibration quality of semantic loss and BEARS on controlled synthetic datasets.


**Related work**:  
* Xu et al., *A Semantic Loss Function for Deep Learning with Symbolic Knowledge*, ICML 2018.
* Marconato et al., *BEARS: Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts*, 2024.

**Advisor**: Sagar Malhotra
