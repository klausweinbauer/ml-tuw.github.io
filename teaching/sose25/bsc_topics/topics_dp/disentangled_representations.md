---
layout: entitled
title: Disentangled Representations
---

**Motivation**: Computing a disentangled representation is a very desirable property for modern deep learning architectures. Having access to individual, disentangled factors is expected to provide significant improvements for generalisation, interpretability and explainability.

**Overview**:

- What is a good representation? (Bengio, et al., "Representation Learning: A Review and New Perspectives", 2013)
- Two common architectures used for disentanglement:
- Variational Auto-Encoders (Kingma &amp; Welling, "Auto-Encoding Variational Bayes", 2013, and "An Introduction to Variational Autoencoders", 2019)
- Generative Adversarial Networks (Goodfellow, et al., "Generative Adversarial Nets", 2014)


**Papers and topics**:

- survey on useful Metrics (Carbonneau, et al., "Measuring Disentanglement: A Review of Metrics", 2022; and Eastwood &amp; Williams, "A Framework for the Quantitative Evaluation of Disentangled Representations", 2018; and Do &amp; Tran, "Theory and Evaluation Metrics for Learning Disentangled Representations", 2019)
- fairness (Creager, et al., "Flexibly Fair Representation Learning by Disentanglement", 2019)
- contrastive Learning (Cao, et al., "An Empirical Study on Disentanglement of Negative-free Contrastive Learning", 2022)
- recommender Systems (Ma, et al., "Learning Disentangled Representations for Recommendation", 2019)
- weakly-Supervised (Locatello, et al., "Weakly-Supervised Disentanglement Without Compromises", 2020)
- semi-supervised (Nie, et al., "Semi-Supervised StyleGAN for Disentanglement Learning", 2020)
