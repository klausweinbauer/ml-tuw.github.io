---
layout: entitled
title: Bachelor Seminar Wissenschaftliches Arbeiten
---

## General information

- TISS: [(link)](https://tiss.tuwien.ac.at/course/courseDetails.xhtml?courseNr=193052&semester=2022W&dswid=5830&dsrid=791)
- contact: [Patrick Indri](mailto:patrick.indri@tuwien.ac.at)
- meeting link: [https://tuwien.zoom.us/my/patrickindri](https://tuwien.zoom.us/my/patrickindri)
- everything important will be announced in TUWEL/TISS.


## Format
This seminar simulates a machine learning conference, where the students take on the role of authors and reviewers. It consists of multiple phases.

### 1. Proposal phase

Attend the **mandatory** first meeting either in person or remotely (details on TUWEL).

#### Option 1: our suggestions
 > You select **two** topics/papers (i.e., two bullet points) from one of the topics below. You will work with the material mentioned in the overview and the topic-specific resources.   


#### Option 2: your own idea + one of our suggestions
 > You choose **your own** topic to work on. This can be some existing machine learning paper/work or an own creative idea in the context of machine learning. We strongly encourage you to start from existing papers from the following venues: NeurIPS, ICML, ICLR, COLT, AISTATS, UAI, JMLR, MLJ. Importantly, your idea has to be specific and worked out well. Nevertheless, choose **one** of our suggestions as well.
 
 
**Independent of the option you chose**, understand the fundamentals of your topic and try to answer the following questions:

- **What** is the problem?
- **Why** is it an interesting problem?
- **How** do you plan to approach the problem? /
**How** have the authors of your topic approached the problem?

Select topics and write a short description of them together with the answers to the questions (~3 sentences should be sufficient) in **TUWEL**.

We can only accept your own proposals if you can answer the mentioned questions and have a well worked out topic.


### 2. Bidding and assignment phase
You and your fellow students will act as reviewers and bid on the topics of your peers you want to review. Based on the biddings, we (in the role as chairs of the conference) will select one of each student's proposals as the actual project you will work on for the rest of this semester. You **do not** need to work on the other project, anymore. Additionally, we will also assign two different projects from other students to you, which you will have to review later in the semester. 

### 3. Working phase
Now the actual work starts. Gather deep understanding of your topic, write a first draft of your report and give a 5-minute presentation. Feel free to go beyond the given material.

You will schedule two meetings with your supervisor to discuss your progress, but do not hesitate to contact him/her if you have any questions.

### 4. Reviewing phase
You will again act as a reviewer for the conference by writing two reviews, one for each draft report assigned to you.

### 5. Writing phase
Based on the reviews from your peers (and our feedback) you will further work on your topic. 

### 6. Submission phase
Give a final presentation and submit your report.

## General resources (freely available books)

- Understanding machine learning: from theory to algorithms. Shai Shalev-Shwartz and Shai Ben-David [(pdf)](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/copy.html)
- Foundations of machine learning. Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar [(pdf)](https://cs.nyu.edu/~mohri/mlbook/)
- Foundations of data science. Avrim Blum, John Hopcroft, and Ravindran Kannan [(pdf)](https://www.cs.cornell.edu/jeh/book.pdf)
- Mathematics for machine learning. Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong [(pdf)](https://mml-book.github.io/)
- Mining of massive datasets. Jure Leskovec, Anand Rajaraman, and Jeffrey D. Ullman [(pdf)](http://infolab.stanford.edu/~ullman/mmds/book0n.pdf)
- Reinforcement learning: an introduction. Richard Sutton and Andrew Barto [(pdf)](http://incompleteideas.net/book/the-book.html)
- Deep learning and neural networks. Ian Goodfellow and Yoshua Bengio and Aaron Courville [(pdf)](https://www.deeplearningbook.org/)

## Topics (Tentative)
You should have access to the literature and papers through Google scholar, DBLP, the provided links, or the TU library. Feel free to watch the linked talks to get an overview on the topics.

<details>
  <summary><b>Kernels</b> (click to expand)</summary>
<p>Overview:</p>
<ul>
<li>preface and introduction up to section 1.5 of "Learning with kernels" by Bernhard Schölkopf and Alex Smola, 2002 <a href="http://agbs.kyb.tuebingen.mpg.de/lwk/">(pdf)</a>.</li>
<li>introduction to kernels: Bernhard Schölkopf - MLSS 2013 <a href="https://www.youtube.com/watch?v=uzWgB1VO9xQ">(youtube-link)</a></li>
</ul>
<p>Papers and topics:</p>
<ul>
<li>support vector machines (Bennett and Campbell. "Support vector machines: hype or hallelujah?." ACM SIGKDD 2000)</li>
<li>one class support vector machine (Khan and Madden. "A survey of recent trends in one class classification." Irish conference on artificial intelligence and cognitive science 2009)</li>
<li>string kernels (Lodhi, et al. "Text classification using string kernels." Journal of machine learning research 2002)</li>
<li>kernels for distances (Schölkopf. "The kernel trick for distances." NIPS 2001)</li>
</ul>

</details>


<details>
  <summary><b>Semi-supervised learning</b> (click to expand)</summary>
  
<p>Overview:</p>
<ul>
<li>chapter 1/introduction of "Semi-supervised learning" by Olivier Chapelle, Bernhard Schölkopf, and Alexander Zien, 2006 <a href="http://olivier.chapelle.cc/ssl-book/ssl_toc.pdf">(pdf)</a>.</li>
<li>introduction to semi-supervised learning: Tom Mitchell - Carnegie Mellon University 2011 <a href="https://www.youtube.com/watch?v=OMRlnKupsXM">(youtube-link)</a></li>
</ul>
<p>Papers and topics:</p>
<ul>
<li>transductive support vector machines (chapter 6 in the "Semi-supervised learning" book mentioned above)</li>
<li>label propagation (chapter 11 in the "Semi-supervised learning" book mentioned above)</li>
<li>randomized min-cuts (Blum, Avrim, et al., "Semi-supervised learning using randomized mincuts.", ICML 2004)</li>
</ul>

</details>

<details>
  <summary><b>Active learning</b> (click to expand)</summary>
  
<p>Overview:</p>
<ul>
<li>chapter 1 "Automating inquiry" of Burr Settles' "Active learning" (AL) book, 2012.</li>
<li>introduction to active learning: Sanjoy Dasgupta - Microsoft 2016 <a href="https://www.youtube.com/watch?v=FE1r7_SQq6Y">(youtube-link)</a></li>
</ul>
<p>Papers and topics:</p>
<ul>
<li>Bayesian active learning on graphs (Ma, Yifet, et al., "σ-optimality for active learning on gaussian random fields." NIPS 2013)</li>
<li>active search on graphs (Wang, Xuezhi, et al., "Active search on graphs" KDD 2013)</li>
<li>shortest-path-based active learning (Dasarathy, et al. "S2: an efficient graph based active learning algorithm with application to nonparametric classification." COLT 2015)</li>
</ul>

</details>


<details>
  <summary><b>Trustworthy ML</b> (click to expand)</summary>
  
<p>Motivation: Machine learning systems are ubiquitous and it is necessary to make sure they behave as intended. In particular, trustworthiness can be achieved by means of privacy-preserving, robust, and explainable algorithms.</p>

  
<p>Overview:</p>
<ul>
  <li>General: What does it mean for ML to be trustworthy? <a href="https://www.youtube.com/watch?v=UpGgIqLhaqo">(youtube-link)</a></li>
  <li>General: Trustworthy ML (Kush R. Varshney) <a href="http://www.trustworthymachinelearning.com/">(link)</a> </li>
  <li>Differential privacy: Chapter 2 of: Dwork, Cynthia, and Aaron Roth. "The algorithmic foundations of differential privacy." Found. Trends Theor. Comput. Sci. 9.3-4 2014 </li>
  <li>Explainability: Samek, Wojciech, and Klaus-Robert Müller. "Towards explainable artificial intelligence." Explainable AI: interpreting, explaining and visualizing deep learning." Springer, Cham, 2019 </li>
</ul>
<p>Papers and topics:</p>
<ul>
  <li>interpreting model predictions</li>
  <ul>
    <li>Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. ""Why should i trust you?" Explaining the predictions of any classifier." ACM SIGKDD 2016</li>
    <li>Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." NeurIPS 2017</li>
  </ul>
  <li>reliability of explanation methods</li>
  <ul>
    <li>Kumar, I. Elizabeth, et al. "Problems with Shapley-value-based explanations as feature importance measures." ICML, 2020.</li>
  </ul>
  <li>robustness against attacks and adversaries</li>
  <ul>
  <li>Jagielski, Matthew, et al. "Manipulating machine learning: Poisoning attacks and countermeasures for regression learning." 2018 IEEE Symposium on Security and Privacy (SP). IEEE, 2018.</li>
  <li>Carmon, Yair, et al. "Unlabeled data improves adversarial robustness." NeurIPS 2019.</li>
  </ul>
  <li>differential privacy</li>
  <ul>
  <li>Abadi, Martin, et al. "Deep learning with differential privacy." Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.</li>
  <li>Patel, Neel, Reza Shokri, and Yair Zick. "Model explanations with differential privacy." 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022.</li>
  </ul>
</ul>

</details>


<details>
  <summary><b>Equivariant neural networks</b> (click to expand)</summary>
  
<p>Motivation: Many datastructures have an innate structure that our neural networks should respect. For example the output of a graph neural networks should not change if we permute the vertices (permutation equivariance/invariance).</p>

<p>Overview:</p>
<ul>
<li>chapter 8 "equivariant neural networks" of "Deep learning for molecules and materials" by Andrew D. White, 2021. <a href="https://whitead.github.io/dmol-book/dl/Equivariant.html">(pdf)</a>.</li>
<li>introduction to equivariance: Taco Cohen and Risi Kondor - Neurips 2020 Tutorial (first half) <a href="https://slideslive.com/38943570/equivariant-networks">(slideslive-link)</a></li>
</ul>
<p>Papers and topics:</p>
<ul>
<li>neural network that can learn on sets (Zaheer, et al. "Deep sets." NeurIPS 2017)</li>
<li>learning equivariance from data (Zhou, et al. "Meta-learning symmetries by reparameterization." ICLR 2021)</li>
</ul>

</details>



<details>
  <summary><b>Graph Neural Networks (GNNs)</b> (click to expand)</summary>
  
<p>Motivation Graphs are a very general structure and can be applied to many areas: molecules and developing medicine, geographical maps, spread of diseases. They can be used to model physical systems and solve partial differential equations. Even images and text can be seen as a special case of graphs. Thus it makes sense to develop neural networks that can work with graphs. GNNs have strong connections to many classical computer science topics (algorithmics, logic, ...) while also making use of neural networks. This  means that work on GNN can be very theoretical, applied or anything in between.</p>

<p>Overview:</p>
<ul>
<li>Sanchez-Lengeling et al. , A Gentle Introduction to Graph Neural Networks, distill.pub 2021</li>
<li>Veličković, Intro to graph neural networks (ML Tech Talks): https://www.youtube.com/watch?v=8owQBFAHw7E  2021</li>
</ul>

<p>Papers and projects:</p>

<ul>
<li>Algorithm representation learning:</li>
<ul>
  <li>(overview of) algorithm representation learning (Veličković et al., The CLRS Algorithmic Reasoning Benchmark, ICML 2022)</li>
  <li>Dudzik and Veličković , Graph Neural Networks are Dynamic Programmers, arXiv 2022: they prove a connection between GNNs and dynamic programs</li>
</ul>
<li>Graph Transformer</li>
<ul>
  <li>transformer architecture on graphs (Kreuzer et al., Rethinking Graph Transformers with Spectral Attention, NeurIPS 2021)</li>
  <li>overview of different architectures in practice (Rampášek et al, Recipe for a General, Powerful, Scalable Graph Transformer, arXiv 2022)</li>
</ul>
<li>Combinatorial Optimization</li>
<ul>
  <li>GNNs for combinatorial problems (Sato et al, Approximation Ratios of Graph Neural Networks for Combinatorial Problems, NeurIPS 2019)</li>
</ul>
</ul>

</details>

<!--
<p style="color:tomato;">Missing: oprimisation (SGD stuff, maybe convex stuff) (Tamara). Multi-view/multi-modal clustering (Tamara). Representation learning (David). Maybe ML for sciences or something. Few "motivation" paragraphs.</p>
-->
